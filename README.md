I have implemented the Self Attention and Multihead attention without masking. In generative langauge models, attention plays an important role to generate the accurate results. I have implemented the attention described in the paper, **"Attention Is All You Need"**.
